from langchain.chains import RetrievalQA
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.llms import Ollama

# ChromaDB ì €ì¥ ê²½ë¡œ
CHROMA_DB_PATH = "C:/rag-project/chroma_db"

# ë²¡í„° DB ë¡œë“œ
embeddings = OllamaEmbeddings(model="mxbai-embed-large")
retriever = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings).as_retriever()
qa_chain = RetrievalQA.from_chain_type(
    llm=Ollama(model="llama3"),
    chain_type="stuff",
    retriever=retriever
)

def answer_question(question):
    """ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë°›ì•„ì„œ ë‹µë³€ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜"""
    if not question.strip():
        return "âŒ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."

    response = qa_chain.invoke({"query": question})
    return response["result"]

# CLIì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ì •
if __name__ == "__main__":
    while True:
        query = input("\nğŸŸ¢ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit' ì…ë ¥): ")
        if query.lower() == "exit":
            print("ğŸ”´ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        print("ğŸ“ ë‹µë³€:", answer_question(query))
